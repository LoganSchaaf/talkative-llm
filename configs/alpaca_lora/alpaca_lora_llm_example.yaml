framework: "alpaca-lora"

mode: "LlamaForCausalLM"
model:
  name: "huggyllama/llama-7b"
  lora_weights: "tloen/alpaca-lora-7b"
  load_8bit: false                       # if device is 'cpu', then load_8bit must be true
device: "cuda"                           # 'cpu' or 'cuda'
batch_size: 20
skip_special_tokens: true

params:
  max_new_tokens: 128
  early_stopping: true
  num_beams: 3
  use_cache: true
  temperature: 1.0
  top_p: 1.0
  top_k: 50
  num_return_sequences: 1